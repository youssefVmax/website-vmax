{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305ba766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ inserts.sql file created with all data\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# -----------------------------\n",
    "# Load Firestore JSON\n",
    "# -----------------------------\n",
    "with open(\"backup.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# -----------------------------\n",
    "# Mapping: Firestore collections → MySQL tables\n",
    "# -----------------------------\n",
    "collection_to_table = {\n",
    "    \"users\": \"users\",\n",
    "    \"targets\": \"salesTargets\",\n",
    "    \"deals\": \"deals\",\n",
    "    \"callbacks\": \"callbacks\",\n",
    "    \"target_progress\": \"targetProgress\",\n",
    "    \"notifications\": \"notifications\"\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Generate SQL inserts\n",
    "# -----------------------------\n",
    "with open(\"inserts.sql\", \"w\", encoding=\"utf-8\") as sql_file:\n",
    "    for collection, table in collection_to_table.items():\n",
    "        if collection not in data:\n",
    "            continue\n",
    "\n",
    "        for doc_id, doc_data in data[collection].items():\n",
    "            fields = []\n",
    "            values = []\n",
    "            for key, value in doc_data.items():\n",
    "                fields.append(f\"`{key}`\")\n",
    "\n",
    "                if value is None:\n",
    "                    values.append(\"NULL\")\n",
    "                elif isinstance(value, str):\n",
    "                    safe_value = value.replace(\"'\", \"''\")  # escape quotes\n",
    "                    values.append(f\"'{safe_value}'\")\n",
    "                else:\n",
    "                    values.append(str(value))\n",
    "\n",
    "            insert_sql = f\"INSERT INTO {table} ({', '.join(fields)}) VALUES ({', '.join(values)});\\n\"\n",
    "            sql_file.write(insert_sql)\n",
    "\n",
    "print(\"✅ inserts.sql file created with all data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07c8aa59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generated:\n",
      " - tables.sql\n",
      " - inserts.sql\n"
     ]
    }
   ],
   "source": [
    "# toSQL: Firestore JSON -> MySQL (phpMyAdmin-friendly)\n",
    "# Generates:\n",
    "#  - tables.sql  (CREATE TABLE statements)\n",
    "#  - inserts.sql (INSERT INTO statements)\n",
    "#\n",
    "# Notes:\n",
    "# - Infers schema by scanning all docs per collection\n",
    "# - Firestore timestamp objects:\n",
    "#     {\"type\":\"firestore/timestamp/1.0\",\"seconds\":..., \"nanoseconds\":...} -> DATETIME\n",
    "# - Strings that match YYYY-MM-DD -> DATE\n",
    "# - Strings that match YYYY-MM-DD HH:MM:SS (or with T, Z) -> DATETIME\n",
    "# - Booleans -> TINYINT(1)\n",
    "# - Numbers: if any float -> DOUBLE, else BIGINT\n",
    "# - Unknown nested objects/arrays -> JSON\n",
    "# - Adds a synthetic `id` (VARCHAR(191)) column as PRIMARY KEY for the doc ID\n",
    "# - All column names and table names are backticked\n",
    "# - Engine=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__)) if \"__file__\" in globals() else os.getcwd()\n",
    "INPUT_JSON = os.path.join(BASE_DIR, \"backup2.json\")  # Ensure this file exists in database/\n",
    "TABLES_SQL = os.path.join(BASE_DIR, \"tables.sql\")\n",
    "INSERTS_SQL = os.path.join(BASE_DIR, \"inserts.sql\")\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers: type detection\n",
    "# -----------------------------\n",
    "DATE_RE = re.compile(r\"^\\d{4}-\\d{2}-\\d{2}$\")\n",
    "DATETIME_RE = re.compile(\n",
    "    r\"^\\d{4}-\\d{2}-\\d{2}[ T]\\d{2}:\\d{2}:\\d{2}(?:\\.\\d+)?(?:Z|[+-]\\d{2}:\\d{2})?$\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "def is_firestore_timestamp(obj):\n",
    "    return (\n",
    "        isinstance(obj, dict)\n",
    "        and obj.get(\"type\") == \"firestore/timestamp/1.0\"\n",
    "        and \"seconds\" in obj\n",
    "    )\n",
    "\n",
    "def normalize_firestore_timestamp(obj):\n",
    "    # Convert seconds (UTC) to 'YYYY-MM-DD HH:MM:SS'\n",
    "    try:\n",
    "        dt = datetime.fromtimestamp(int(obj[\"seconds\"]), tz=timezone.utc)\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def detect_scalar_type(value):\n",
    "    # Returns one of: 'null', 'bool', 'int', 'float', 'date', 'datetime', 'string', 'timestamp', 'json'\n",
    "    if value is None:\n",
    "        return \"null\"\n",
    "    if is_firestore_timestamp(value):\n",
    "        return \"timestamp\"\n",
    "    if isinstance(value, bool):\n",
    "        return \"bool\"\n",
    "    if isinstance(value, int):\n",
    "        return \"int\"\n",
    "    if isinstance(value, float):\n",
    "        return \"float\"\n",
    "    if isinstance(value, str):\n",
    "        s = value.strip()\n",
    "        if DATE_RE.match(s):\n",
    "            return \"date\"\n",
    "        if DATETIME_RE.match(s):\n",
    "            return \"datetime\"\n",
    "        return \"string\"\n",
    "    # arrays or objects (non-timestamp): treat as JSON\n",
    "    if isinstance(value, (list, dict)):\n",
    "        return \"json\"\n",
    "    # fallback\n",
    "    return \"string\"\n",
    "\n",
    "def merge_types(existing, new):\n",
    "    # Merge detected types for a field across documents\n",
    "    # Priority order for final SQL typing:\n",
    "    #  - If any json -> JSON\n",
    "    #  - Else if any timestamp -> DATETIME\n",
    "    #  - Else if any datetime -> DATETIME\n",
    "    #  - Else if any date -> DATE (unless datetime present)\n",
    "    #  - Else if any float -> DOUBLE\n",
    "    #  - Else if any int -> BIGINT\n",
    "    #  - Else if any bool -> TINYINT(1)\n",
    "    #  - Else string -> TEXT\n",
    "    if existing is None:\n",
    "        return {new}\n",
    "    existing.add(new)\n",
    "    return existing\n",
    "\n",
    "def choose_sql_type(types_set, field_name=\"\"):\n",
    "    types = types_set or set([\"string\"])\n",
    "    # JSON dominates\n",
    "    if \"json\" in types:\n",
    "        return \"JSON\"\n",
    "    if \"timestamp\" in types or \"datetime\" in types:\n",
    "        return \"DATETIME\"\n",
    "    if \"date\" in types and \"datetime\" not in types and \"timestamp\" not in types:\n",
    "        return \"DATE\"\n",
    "    if \"float\" in types:\n",
    "        return \"DOUBLE\"\n",
    "    if \"int\" in types:\n",
    "        return \"BIGINT\"\n",
    "    if \"bool\" in types:\n",
    "        return \"TINYINT(1)\"\n",
    "    # Fallback to TEXT for strings/mixed\n",
    "    # You can customize some known fields to VARCHAR(191)\n",
    "    if field_name in {\"email\", \"username\", \"name\", \"team\", \"role\", \"sales_team\", \"service_tier\", \"stage\", \"priority\", \"country\", \"customer_name\", \"phone_number\", \"DealID\", \"device_id\", \"device_key\", \"created_by\", \"created_by_id\", \"SalesAgentID\", \"ClosingAgentID\", \"sales_agent\", \"closing_agent\", \"sales_agent_norm\", \"closing_agent_norm\", \"managerId\", \"managerName\", \"agentId\", \"agentName\", \"period\"}:\n",
    "        return \"VARCHAR(191)\"\n",
    "    return \"TEXT\"\n",
    "\n",
    "def sql_escape_string(s):\n",
    "    # Escape single quotes by doubling them, and backslashes\n",
    "    return s.replace(\"\\\\\", \"\\\\\\\\\").replace(\"'\", \"''\")\n",
    "\n",
    "def value_to_sql_literal(value, sql_type):\n",
    "    if value is None:\n",
    "        return \"NULL\"\n",
    "    if sql_type == \"TINYINT(1)\":\n",
    "        # Boolean or small int\n",
    "        if isinstance(value, bool):\n",
    "            return \"1\" if value else \"0\"\n",
    "        return \"1\" if str(value).lower() == \"true\" else (\"0\" if str(value).lower() == \"false\" else str(int(value)))\n",
    "    if sql_type in (\"BIGINT\", \"DOUBLE\"):\n",
    "        try:\n",
    "            if isinstance(value, bool):\n",
    "                return \"1\" if value else \"0\"\n",
    "            if sql_type == \"BIGINT\":\n",
    "                return str(int(value))\n",
    "            return str(float(value))\n",
    "        except Exception:\n",
    "            # fallback to quoted string\n",
    "            return f\"'{sql_escape_string(str(value))}'\"\n",
    "    if sql_type == \"DATE\":\n",
    "        if isinstance(value, str) and DATE_RE.match(value.strip()):\n",
    "            return f\"'{value.strip()}'\"\n",
    "        # Best-effort format\n",
    "        return \"NULL\"\n",
    "    if sql_type == \"DATETIME\":\n",
    "        if is_firestore_timestamp(value):\n",
    "            norm = normalize_firestore_timestamp(value)\n",
    "            return f\"'{norm}'\" if norm else \"NULL\"\n",
    "        if isinstance(value, str):\n",
    "            s = value.strip().replace(\"T\", \" \").replace(\"Z\", \"\")\n",
    "            # Attempt parse\n",
    "            try:\n",
    "                # Normalize to 'YYYY-MM-DD HH:MM:SS'\n",
    "                dt = datetime.fromisoformat(s)\n",
    "                return f\"'{dt.strftime('%Y-%m-%d %H:%M:%S')}'\"\n",
    "            except Exception:\n",
    "                # If already \"YYYY-MM-DD HH:MM:SS\"\n",
    "                if DATETIME_RE.match(s):\n",
    "                    # If it includes offset, we could parse; otherwise, accept as is\n",
    "                    parts = s.split(\".\")[0]\n",
    "                    return f\"'{parts}'\" if \" \" in parts else f\"'{parts} 00:00:00'\"\n",
    "        return \"NULL\"\n",
    "    if sql_type == \"JSON\":\n",
    "        try:\n",
    "            return f\"'{sql_escape_string(json.dumps(value, ensure_ascii=False))}'\"\n",
    "        except Exception:\n",
    "            return \"NULL\"\n",
    "    # TEXT and VARCHAR\n",
    "    s = str(value)\n",
    "    return f\"'{sql_escape_string(s)}'\"\n",
    "\n",
    "# -----------------------------\n",
    "# Load data\n",
    "# -----------------------------\n",
    "with open(INPUT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Collections may be dynamic; we’ll iterate all top-level keys that are dicts\n",
    "collections = {k: v for k, v in data.items() if isinstance(v, dict)}\n",
    "\n",
    "# -----------------------------\n",
    "# Infer schemas per collection\n",
    "# -----------------------------\n",
    "schemas = {}  # {table_name: {field_name: sql_type}}\n",
    "order_of_fields = {}  # preserve a deterministic order\n",
    "for coll_name, docs in collections.items():\n",
    "    # Table name = collection name as-is\n",
    "    table = coll_name\n",
    "    field_types = {}       # {field_name: set(types)}\n",
    "    fields_seen_order = [] # deterministic order\n",
    "    \n",
    "    # We will always include an 'id' column for the document ID\n",
    "    field_types[\"id\"] = {\"string\"}  # id as string\n",
    "    fields_seen_order.append(\"id\")\n",
    "    \n",
    "    for doc_id, doc_data in docs.items():\n",
    "        if not isinstance(doc_data, dict):\n",
    "            # Skip malformed items\n",
    "            continue\n",
    "        # Merge keys\n",
    "        for k, v in doc_data.items():\n",
    "            t = detect_scalar_type(v)\n",
    "            if k not in field_types:\n",
    "                field_types[k] = set()\n",
    "                fields_seen_order.append(k)\n",
    "            field_types[k] = merge_types(field_types[k], t)\n",
    "    \n",
    "    # Choose SQL types\n",
    "    sql_field_types = {}\n",
    "    for k in fields_seen_order:\n",
    "        sql_field_types[k] = choose_sql_type(field_types.get(k), field_name=k)\n",
    "    \n",
    "    # Ensure 'id' is VARCHAR(191)\n",
    "    sql_field_types[\"id\"] = \"VARCHAR(191)\"\n",
    "    \n",
    "    schemas[table] = sql_field_types\n",
    "    order_of_fields[table] = fields_seen_order\n",
    "\n",
    "# -----------------------------\n",
    "# Generate CREATE TABLE statements\n",
    "# -----------------------------\n",
    "create_statements = []\n",
    "for table, fields in schemas.items():\n",
    "    cols = []\n",
    "    for fname in order_of_fields[table]:\n",
    "        sql_type = fields[fname]\n",
    "        nullability = \"NULL\"  # allow NULLs by default\n",
    "        # You can mark id as NOT NULL\n",
    "        if fname == \"id\":\n",
    "            nullability = \"NOT NULL\"\n",
    "        cols.append(f\"  `{fname}` {sql_type} {nullability}\")\n",
    "    cols.append(\"  PRIMARY KEY (`id`)\")\n",
    "    create_sql = (\n",
    "        f\"CREATE TABLE IF NOT EXISTS `{table}` (\\n\"\n",
    "        + \",\\n\".join(cols)\n",
    "        + \"\\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;\"\n",
    "    )\n",
    "    create_statements.append(create_sql)\n",
    "\n",
    "with open(TABLES_SQL, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"-- Auto-generated by toSQL notebook\\n\\n\")\n",
    "    f.write(\"SET NAMES utf8mb4;\\nSET time_zone = '+00:00';\\n\\n\")\n",
    "    for stmt in create_statements:\n",
    "        f.write(stmt + \"\\n\\n\")\n",
    "\n",
    "# -----------------------------\n",
    "# Generate INSERT statements\n",
    "# -----------------------------\n",
    "with open(INSERTS_SQL, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"-- Auto-generated by toSQL notebook\\n\\n\")\n",
    "    for table, docs in collections.items():\n",
    "        fields_order = order_of_fields[table]\n",
    "        # Emit rows\n",
    "        for doc_id, doc_data in docs.items():\n",
    "            if not isinstance(doc_data, dict):\n",
    "                continue\n",
    "            values_sql = []\n",
    "            for fname in fields_order:\n",
    "                sql_type = schemas[table][fname]\n",
    "                if fname == \"id\":\n",
    "                    values_sql.append(value_to_sql_literal(doc_id, \"VARCHAR(191)\"))\n",
    "                else:\n",
    "                    raw_val = doc_data.get(fname)\n",
    "                    values_sql.append(value_to_sql_literal(raw_val, sql_type))\n",
    "            insert_sql = f\"INSERT INTO `{table}` ({', '.join(f'`{c}`' for c in fields_order)}) VALUES ({', '.join(values_sql)});\"\n",
    "            f.write(insert_sql + \"\\n\")\n",
    "\n",
    "print(\"✅ Generated:\")\n",
    "print(f\" - {os.path.relpath(TABLES_SQL, BASE_DIR)}\")\n",
    "print(f\" - {os.path.relpath(INSERTS_SQL, BASE_DIR)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
